{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In order to debug the torch transfer of weights, we're gonna side by side compare the forward passes of the two models... :("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with loading keras generator network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# point path to genesis repo\n",
    "sys.path.insert(\n",
    "    0,\n",
    "    '/gpfs/commons/home/tchen/al_project/genesis/analysis/splicing'\n",
    ")\n",
    "from definitions.generator.splirent_deconv_conv_generator_concat import load_generator_network\n",
    "from genesis.generator import build_generator, st_sampled_softmax, st_hardmax_softmax\n",
    "from pathlib import Path\n",
    "from keras.models import load_model\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Input, Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /gpfs/commons/home/tchen/al_project/combined_pytorch_keras_env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /gpfs/commons/home/tchen/al_project/combined_pytorch_keras_env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /gpfs/commons/home/tchen/al_project/combined_pytorch_keras_env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4185: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /gpfs/commons/home/tchen/al_project/combined_pytorch_keras_env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:245: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /gpfs/commons/home/tchen/al_project/combined_pytorch_keras_env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /gpfs/commons/home/tchen/al_project/combined_pytorch_keras_env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /gpfs/commons/home/tchen/al_project/combined_pytorch_keras_env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /gpfs/commons/home/tchen/al_project/combined_pytorch_keras_env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /gpfs/commons/home/tchen/al_project/combined_pytorch_keras_env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /gpfs/commons/home/tchen/al_project/combined_pytorch_keras_env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /gpfs/commons/home/tchen/al_project/combined_pytorch_keras_env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From /gpfs/commons/home/tchen/al_project/combined_pytorch_keras_env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-10 22:26:48.098985: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\n",
      "2024-03-10 22:26:48.108600: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2400000000 Hz\n",
      "2024-03-10 22:26:48.110813: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x564dc63bf6d0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2024-03-10 22:26:48.110841: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /gpfs/commons/home/tchen/al_project/combined_pytorch_keras_env/lib/python3.7/site-packages/genesis/generator/genesis_generator.py:26: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.random.categorical` instead.\n",
      "WARNING:tensorflow:From /gpfs/commons/home/tchen/al_project/combined_pytorch_keras_env/lib/python3.7/site-packages/genesis/generator/genesis_generator.py:28: The name tf.ceil is deprecated. Please use tf.math.ceil instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = 'genesis_splicing_cnn_target_isoform_00_pwm_and_multisample_hek_only_random_regions_50_epochs_harderentropy_generator.h5'\n",
    "model_save_dir = '/gpfs/commons/groups/knowles_lab/ting/DEN_splicing_pretrained_models/'\n",
    "\n",
    "full_path = model_save_dir + model_name\n",
    "generator_model = load_model(filepath=str(full_path), custom_objects={'K': K, 'st_sampled_softmax': st_sampled_softmax, 'st_hardmax_softmax': st_hardmax_softmax}, compile=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load the torch model with the previously saved weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# point path to our repo\n",
    "sys.path.insert(\n",
    "    0,\n",
    "    '/gpfs/commons/home/tchen/al_project/active-learning-cnns-gps/src'\n",
    ")\n",
    "\n",
    "import numpy as np\n",
    "from models.den import Generator\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '/gpfs/commons/groups/knowles_lab/ting/DEN_splicing_generator_weights/'\n",
    "save_name = 'target_isoform_00.npy'\n",
    "\n",
    "target_isoform_net_weights = np.load(save_path+save_name, allow_pickle=True)\n",
    "\n",
    "# list of numpy arrays\n",
    "weights = target_isoform_net_weights.tolist()\n",
    "device = torch.device('cpu')\n",
    "\n",
    "# need to extract embedding template and mask from pretrained network\n",
    "pretrained_embedding_template = torch.tensor(weights[-2]).to(device)\n",
    "pretrained_embedding_mask = torch.tensor(weights[-1]).to(device)\n",
    "\n",
    "n_samples = 10\n",
    "n_classes = 1\n",
    "seq_length = 109\n",
    "latent_dim = 100\n",
    "batch_size = 32\n",
    "\n",
    "torch_generator = Generator(embedding_template=pretrained_embedding_template,\n",
    "                                   embedding_mask=pretrained_embedding_mask,\n",
    "                                   device=device,\n",
    "                                   latent_dim=latent_dim,\n",
    "                                   batch_size=batch_size,\n",
    "                                   seq_length=seq_length,\n",
    "                                   n_classes=n_classes,\n",
    "                                   n_samples=n_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "running_keras_weight_idx = 0\n",
    "\n",
    "for i, layer in enumerate(torch_generator.generator_network.generator_network):\n",
    "    if isinstance(layer, nn.Linear):\n",
    "        # transfer linear layer weights and biases\n",
    "        assert tuple(layer.weight.data.shape) == weights[running_keras_weight_idx].T.shape\n",
    "        layer.weight.data = torch.from_numpy(weights[running_keras_weight_idx].T)\n",
    "        running_keras_weight_idx += 1\n",
    "\n",
    "        assert layer.bias.data.shape == weights[running_keras_weight_idx].shape\n",
    "        layer.bias.data = torch.from_numpy(weights[running_keras_weight_idx])\n",
    "        running_keras_weight_idx += 1\n",
    "    elif isinstance(layer, nn.ConvTranspose2d):\n",
    "        # transfer convtranspose2d weights and biases\n",
    "        assert tuple(layer.weight.data.shape) == np.transpose(weights[running_keras_weight_idx], axes=[3, 2, 0, 1]).shape\n",
    "        layer.weight.data = torch.from_numpy(np.transpose(weights[running_keras_weight_idx], axes=[3, 2, 0, 1]))\n",
    "        running_keras_weight_idx += 1\n",
    "        \n",
    "        assert layer.bias.data.shape == weights[running_keras_weight_idx].shape\n",
    "        layer.bias.data = torch.from_numpy(weights[running_keras_weight_idx])\n",
    "        running_keras_weight_idx += 1\n",
    "    elif isinstance(layer, nn.Conv2d):\n",
    "        # transfer conv2d weights and biases\n",
    "        assert tuple(layer.weight.data.shape) == np.transpose(weights[running_keras_weight_idx], axes=[3, 2, 0, 1]).shape\n",
    "        layer.weight.data = torch.from_numpy(np.transpose(weights[running_keras_weight_idx], axes=[3, 2, 0, 1]))\n",
    "        running_keras_weight_idx += 1\n",
    "        \n",
    "        assert layer.bias.data.shape == weights[running_keras_weight_idx].shape\n",
    "        layer.bias.data = torch.from_numpy(weights[running_keras_weight_idx])\n",
    "        running_keras_weight_idx += 1\n",
    "    elif isinstance(layer, nn.BatchNorm2d):\n",
    "        # transfer batch norm gamma, beta, running mean, running variance\n",
    "        # order from keras model should be gamma, beta, moving mean, moving variance\n",
    "        assert tuple(layer.weight.data.shape) == weights[running_keras_weight_idx].shape # this might need a transpose\n",
    "        layer.weight.data = torch.from_numpy(weights[running_keras_weight_idx])\n",
    "        running_keras_weight_idx += 1\n",
    "        \n",
    "        assert layer.bias.data.shape == weights[running_keras_weight_idx].shape\n",
    "        layer.bias.data = torch.from_numpy(weights[running_keras_weight_idx])\n",
    "        running_keras_weight_idx += 1\n",
    "        \n",
    "        assert layer.running_mean.data.shape == weights[running_keras_weight_idx].shape\n",
    "        layer.running_mean.data = torch.from_numpy(weights[running_keras_weight_idx])\n",
    "        running_keras_weight_idx += 1\n",
    "        \n",
    "        assert layer.running_var.data.shape == weights[running_keras_weight_idx].shape\n",
    "        layer.running_var.data = torch.from_numpy(weights[running_keras_weight_idx])\n",
    "        running_keras_weight_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (generator_network): GeneratorNetwork(\n",
       "    (generator_network): ModuleList(\n",
       "      (0): Linear(in_features=101, out_features=3456, bias=True)\n",
       "      (1): ConvTranspose2d(384, 256, kernel_size=(7, 1), stride=(2, 1))\n",
       "      (2): BatchNorm2d(256, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)\n",
       "      (3): ConvTranspose2d(256, 192, kernel_size=(8, 1), stride=(2, 1))\n",
       "      (4): BatchNorm2d(192, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)\n",
       "      (5): ConvTranspose2d(192, 128, kernel_size=(7, 1), stride=(2, 1))\n",
       "      (6): BatchNorm2d(128, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)\n",
       "      (7): Conv2d(128, 128, kernel_size=(8, 1), stride=(1, 1), padding=same)\n",
       "      (8): BatchNorm2d(128, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)\n",
       "      (9): Conv2d(128, 64, kernel_size=(8, 1), stride=(1, 1), padding=same)\n",
       "      (10): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)\n",
       "      (11): Conv2d(64, 4, kernel_size=(8, 1), stride=(1, 1), padding=same)\n",
       "    )\n",
       "  )\n",
       "  (onehot_template_layer): Embedding(1, 436)\n",
       "  (onehot_mask_layer): Embedding(1, 436)\n",
       "  (straight_through): StraightThroughEstimator()\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_generator.to(device)\n",
    "torch_generator.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay now we'll start with the same input and layer by layer compare the outputs of the keras model with the outputs of the torch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator forward loop (up until generator network)\n",
    "\n",
    "# Seed class input for all dense/embedding layers\n",
    "sequence_class = torch.empty(batch_size).to(device)\n",
    "\n",
    "# TODO: Super jank, refactor this...\n",
    "sequence_class.uniform_(-0.499, 1-0.5001)\n",
    "sequence_class = torch.round(sequence_class).int()\n",
    "# sequence_class = torch.ones(self.batch_size, dtype=torch.int32).to(self.device)\n",
    "\n",
    "# Get generated policy pwm logits (non-masked)\n",
    "# latent_input_1 = torch.empty((batch_size, latent_dim)).to(device)\n",
    "# latent_input_2 = torch.empty((batch_size, latent_dim)).to(device)\n",
    "latent_input_1 = torch.ones((batch_size, latent_dim)).to(device)\n",
    "latent_input_2 = torch.ones((batch_size, latent_dim)).to(device)\n",
    "\n",
    "# latent_input_1.uniform_(-1.0, 1.0)\n",
    "# latent_input_2.uniform_(-1.0, 1.0)\n",
    "\n",
    "sequence_class_onehots = torch.eye(1).to(device)\n",
    "\n",
    "# check here for bugs\n",
    "class_embedding = sequence_class_onehots.index_select(0, index=sequence_class)    # tf.gather equivalent\n",
    "# class_embedding = torch.zeros(self.batch_size, 1, dtype=torch.int32).to(self.device)\n",
    "\n",
    "torch_seed_input_1 = torch.cat([latent_input_1, class_embedding], dim=-1)\n",
    "torch_seed_input_2 = torch.cat([latent_input_2, class_embedding], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Concatenate, Reshape, Softmax, Conv2DTranspose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat same for keras model\n",
    "sequence_class_input = Input(tensor=K.ones((batch_size, 1)), dtype='int32', name='sequence_class_seed')\n",
    "sequence_class = Lambda(lambda inp: K.cast(K.round(inp * K.random_uniform((batch_size, 1), minval=-0.4999, maxval=n_classes-0.5001)), dtype='int32'), name='lambda_rand_sequence_class')(sequence_class_input)\n",
    "\n",
    "sequence_class_onehots = np.eye(n_classes)\n",
    "\n",
    "#Generator network parameters\n",
    "latent_size = 100\n",
    "\n",
    "#Generator inputs\n",
    "latent_input_1 = Input(tensor=K.ones((batch_size, latent_size)), name='noise_input_1')\n",
    "latent_input_2 = Input(tensor=K.ones((batch_size, latent_size)), name='noise_input_2')\n",
    "# latent_input_1_out = Lambda(lambda inp: inp * K.random_uniform((batch_size, latent_size), minval=-1.0, maxval=1.0), name='lambda_rand_input_1')(latent_input_1)\n",
    "# latent_input_2_out = Lambda(lambda inp: inp * K.random_uniform((batch_size, latent_size), minval=-1.0, maxval=1.0), name='lambda_rand_input_2')(latent_input_2)\n",
    "\n",
    "class_embedding = Lambda(lambda x: K.gather(K.constant(sequence_class_onehots), K.cast(x[:, 0], dtype='int32')))(sequence_class)\n",
    "\n",
    "keras_seed_input_1 = Concatenate(axis=-1)([latent_input_1, class_embedding])\n",
    "keras_seed_input_2 = Concatenate(axis=-1)([latent_input_2, class_embedding])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "func = K.function([sequence_class_input, latent_input_1, latent_input_2], [seed_input_1, seed_input_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_sequence_class = func([K.ones((batch_size, 1)), K.ones((batch_size, latent_size)), K.ones((batch_size, latent_size))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       ...,\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras_sequence_class[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All good so far from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "combined_pytorch_keras_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
